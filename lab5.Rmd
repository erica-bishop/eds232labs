---
title: "Lab 5 - part 1"
author: "Erica Bishop (and Jillian Allison)"
date: "2023-02-07"
output: pdf_document
---

This week's lab is a musical lab. You'll be requesting data from the Spotify API and using it to build k-nearest neighbor and decision tree models.

In order to use the Spotify you must have a Spotify account. If you don't have one, sign up for a free one here: <https://www.spotify.com/us/signup>

Once you have an account, go to Spotify for developers (<https://developer.spotify.com/>) and log in. Click the green "Create a Client ID" button to fill out the form to create an app create an app so you can access the API.

On your developer dashboard page, click on the new app you just created. On the app's dashboard page you will find your Client ID just under the header name of your app. Click "Show Client Secret" to access your secondary Client ID. When you do this you'll be issued a Spotify client ID and client secret key.

You have two options for completing this lab.

**Option 1**: **Classify by users**. Build models that predict whether a given song will be in your collection vs. a partner in class. This requires that you were already a Spotify user so you have enough data to work with. You will download your data from the Spotify API and then exchange with another member of class.

**Option 2**: **Classify by genres**. Build models that predict which genre a song belongs to. This will use a pre-existing Spotify dataset available from Kaggle.com (<https://www.kaggle.com/datasets/mrmorj/dataset-of-songs-in-spotify>)

```{r}
library(spotifyr) #API interaction
library(tidyverse)
library(tidymodels)
library(gt)
library(here)
library(patchwork)
library(tune)
library(baguette)
library(doParallel)
library(ranger)

```

Client ID and Client Secret are required to create and access token that is required to interact with the API. You can set them as system values so we don't have to do provide them each time.

Sys.setenv(SPOTIFY_CLIENT_ID = '2e066a0ebf86473990f30eee626c4ab9') Sys.setenv(SPOTIFY_CLIENT_SECRET = 'bb6735cb728a42089e0cc02680c97844')

access_token \<- get_spotify_access_token() #takes ID and SECRET, sends to Spotify and receives an access token

> *This may result in an error:*
>
> INVALID_CLIENT: Invalid redirect URI
>
> *This can be resolved by editing the callback settings on your app. Go to your app and click "Edit Settings". Under redirect URLs paste this: <http://localhost:1410/> and click save at the bottom.*

**Option 1: Data Preparation**

You can use get_my_saved_tracks() to request all your liked tracks. It would be good if you had at least 150-200 liked tracks so the model has enough data to work with. If you don't have enough liked tracks, you can instead use get_my_recently_played(), and in that case grab at least 500 recently played tracks if you can.

The Spotify API returns a dataframe of tracks and associated attributes. However, it will only return up to 50 (or 20) tracks at a time, so you will have to make multiple requests. Use a function to combine all your requests in one call.

Once you have your tracks, familiarize yourself with this initial dataframe. You'll need to request some additional information for the analysis. If you give the API a list of track IDs using get_track_audio_features(), it will return an audio features dataframe of all the tracks and some attributes of them.

These track audio features are the predictors we are interested in, but this dataframe doesn't have the actual names of the tracks. Append the 'track.name' column from your favorite tracks database.

Find a class mate whose data you would like to use. Add your partner's data to your dataset. Create a new column that will contain the outcome variable that you will try to predict. This variable should contain two values that represent if the track came from your data set or your partner's.

**Option 2: Data preparation**

Download the Spotify dataset from <https://www.kaggle.com/datasets/mrmorj/dataset-of-songs-in-spotify>

Inspect the data. Choose two genres you'd like to use for the classification task. Filter down the data to include only the tracks of that genre.

**see the r script lab5_get_spotify_data.R for accessing data via spotify API.** Below I read in the data I pulled from spotify, and saved the API calls and data manipulation in a separate script to make knitting faster for this markdown file.

```{r load_data}

#read in data (saved as csv from r script code)
eb_tracks_df <- read_csv(here("eb_track_attributes.csv"))

#read in Jillian's data
ja_tracks_df <- read_csv(here("jillian_spotify_data.csv")) |> 
  select(-track_id) |> #drop track_id column (repeats id column)
  rename(track.name = track_name)#renametrack_name to match

```

##Data Exploration (both options)

Let's take a look at your data. Do some exploratory summary stats and visualization.

For example: What are the most danceable tracks in your dataset? What are some differences in the data between users (Option 1) or genres (Option 2)?

```{r}

#skim both datasets
skimr::skim(eb_tracks_df)
skimr::skim(ja_tracks_df)

#take a closer look at some variables
names(eb_tracks_df)
names(ja_tracks_df)

#do we have any of the same tracks saved??
eb_ja_shared_faves <- semi_join(eb_tracks_df, ja_tracks_df, by = "id") #use semi_join to get df of tracks in common
#looks like a lot of beyonce, frank ocean, and lady gaga, but let's take a closer look!

#what are the characteristics of our shared tracks?
eb_ja_summary <- eb_ja_shared_faves |> 
  select(danceability, energy, key, valence, loudness, speechiness, acousticness) |> 
  summarise_all(mean) |> #calculate the average of some variables of interest
  add_column(user = "both <3") #update user column to show both of us

#compare the characteristics of shared tracks to just mine
eb_summary <- eb_tracks_df |> 
  select(danceability, energy, key, valence, loudness, speechiness, acousticness) |> 
  summarise_all(mean) |> 
  add_column(user = "Erica")

#calculate same summary table for jillian
ja_summary <- ja_tracks_df |> 
  select(danceability, energy, key, valence, loudness, speechiness, acousticness) |> 
  summarise_all(mean) |> 
  add_column(user = "Jillian")

#combine the summary stats tables into one df 
summary_df <- rbind(eb_ja_summary, eb_summary, ja_summary)

gt(summary_df) #print as table

```


```{r}

#combine both data sets

eb_ja_tracks <- full_join(eb_tracks_df,
                          ja_tracks_df,
                          by = c("danceability", "energy", "key", "loudness", "mode", "speechiness", "acousticness", "instrumentalness", "liveness", "valence", "tempo", "type", "id", "uri", "track_href", "analysis_url", "duration_ms", "time_signature", "track.name")) |> 
  mutate(user = case_when(
    user.y == "Jillian" & user.x == "Erica" ~ "Both", #relabel songs in both user lists with case_when
    user.y == "Jillian" & is.na(user.x) ~ "Jillian",
    TRUE ~ "Erica"
  )) |>  #drop individual user columns and a few other less useful predictors
  select(-c(user.x, user.y, uri, track_href, analysis_url, type))

#save output df as a csv (in case api causes issues again)
write_csv(eb_ja_tracks, here("eb_ja_tracks.csv"))

```


```{r}
### compare the spread of some key variables 

#compare danceability
dance_plot <- ggplot(data = eb_ja_tracks,
       aes(y = danceability,
           x = user,
           col = user)) +
  geom_boxplot() +
  labs(title = "Daceability")

#compare energy
energy_plot <- ggplot(data = eb_ja_tracks,
       aes(y = energy,
           x = user,
           col = user)) +
  geom_boxplot() +
  labs(title = "Energy")

#compare key
key_plot <- ggplot(data = eb_ja_tracks,
       aes(y = key,
           x = user,
           col = user)) +
  geom_boxplot() +
  labs(title = "Key")

#compare valence
valence_plot <- ggplot(data = eb_ja_tracks,
       aes(y = valence,
           x = user,
           col = user)) +
  geom_boxplot() +
  labs(title = "Valence")

#compare tempo
tempo_plot <- ggplot(data = eb_ja_tracks,
       aes(y = tempo,
           x = user,
           col = user)) +
  geom_boxplot() +
  labs(title = "Tempo")

#compare mode

mode_plot <- ggplot(data = eb_ja_tracks,
       aes(y = loudness,
           x = user,
           col = user)) +
  geom_boxplot() +
  labs(title = "Loudness")

dance_plot + energy_plot + key_plot + valence_plot + tempo_plot + mode_plot +
  plot_layout(guides = 'collect')


```
Based on how similar these variables are across Jillian and me, it could be very tough for the models to correctly classify tracks - but we'll see!

### **Modeling**
#Now with an added random forest component and some clarification your task.

Create competing models that predict whether a track belongs to:

**Option 1.  you or your partner's collection**

Option 2.  genre 1 or genre 2

Create three final candidate models:

1. k-nearest neighbor 
2. decision tree
3. bagged tree
    - bag_tree()
    - Use the "times =" argument when setting the engine during model specification to specify the number of         trees. The rule of thumb is that 50-500 trees is usually sufficient.  The bottom of that range should          be sufficient here.  
4. random forest
    - rand_forest()
    - m_try() is the new hyperparameter of interest for this type of model.  Make sure to include it in your         tuning process

Go through the modeling process for each model:

Preprocessing. You can use the same recipe for all the models you create. 

Resampling. Make sure to use appropriate resampling to select the best version created by each algorithm.

Tuning. Find the best values for each hyperparameter (within a reasonable range).

Compare the performance of the four final models you have created.  

Use appropriate performance evaluation metric(s) for this classification task. A table would be a good way to display your comparison.  Use at least one visualization illustrating your model results

## Pre-Processing
Splitting the data and encoding - this pre-processed data will be used in each of the four models below.

```{r preprocessing}
#remove unique identifiers from dataset (track id and song name)
tracks_df <- eb_ja_tracks |> 
  select(-c(id, track.name)) |> 
  mutate(user = as.factor(user)) #make outcome variable a factor

#split the data FIRST
set.seed(123) #set seed

tracks_split <- initial_split(tracks_df) #split
tracks_train <- training(tracks_split) #training dataset
tracks_test <- testing(tracks_split) #testing dataset

#Preprocess the data - encoding with the recipe
tracks_recipe <- recipe(user ~., data = tracks_train) |> 
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) |> #one hot encoding of nominal variables (- user)
  step_normalize(all_numeric(), -all_outcomes()) |> #normalize scale of numeric variables 
  prep()


```

Create re-sampling folds 

```{r resample_folds}

#resample
## This resampling can be used for all of the models below!
set.seed(123)
cv_folds <-tracks_train |> 
  vfold_cv() #10 folds is default

```

## KNN Model

```{r knn_wf}
#specify model
knn_spec <- nearest_neighbor() |> #use defaults for first specification
  set_engine("kknn") |> 
  set_mode("classification") 

#create a workflow
knn_workflow <- workflow() |> 
  add_model(knn_spec) |> 
  add_recipe(tracks_recipe)


```

```{r knn_tune}

#fit resamples to the workflow
knn_res <- 
  knn_workflow |> 
  fit_resamples( #tuning function
    resamples = cv_folds,
    control = control_resamples(save_pred = TRUE) #save prediction
    )

# Now define our KNN model with tuning
knn_spec_tune <- 
  nearest_neighbor(neighbors = tune()) |>  #now specify how many neighbors - use tune to look for best
  set_mode("classification") |> 
  set_engine("kknn")

# Define a new workflow
wf_knn_tune <- workflow() |> 
  add_model(knn_spec_tune) |> 
  add_recipe(tracks_recipe) 

# Fit the workflow on our predefined folds and hyperparameters
fit_knn_cv <- wf_knn_tune |> 
  tune_grid( 
    cv_folds,
    grid = data.frame(neighbors = c(1, 5, seq(10,100,10)))) 

```


```{r knn_metrics}

# The final workflow for our KNN model
final_knn_wf <- 
  knn_workflow |> 
  finalize_workflow(select_best(fit_knn_cv))


# # Fitting our final workflow
final_knn_fit <- final_knn_wf |> 
  last_fit(tracks_split) 

# Collect metrics
knn_metrics <- final_knn_fit |> collect_metrics()

```


## Decision Tree

```{r dtree_tune}

#create a model with with tuning specifications
tree_spec_tune <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = tune()
) |> 
  set_engine("rpart") |> 
  set_mode("classification")

#test out some hyperparameters in a search grid
tree_grid <- grid_regular(cost_complexity(), tree_depth(),
                          min_n(),
                          levels = 3) #27 total outcomes to try

tree_grid


```

```{r}

#add tree specifications to a workflow
wf_tree_tune <- workflow() |> 
  add_recipe(tracks_recipe) |> 
  add_model(tree_spec_tune)


doParallel::registerDoParallel() #build trees in parallel

tree_rs <- tune_grid(
  tree_spec_tune,
  user~.,
  resamples = cv_folds,
  grid = tree_grid,
  metrics = metric_set(accuracy)
)

tree_rs

#check out resampled models

autoplot(tree_rs) +
  theme_minimal()

```

```{r}

show_best(tree_rs) #show me the best tree! 
select_best(tree_rs) #pick the best tree

```
As shown in the plot above and the output from the `select_best` tree function, my best model has a tree depth of just one. I suspect this is because the tracks in the two user groups are so similar so the greater depth is adding variance and making the model worse.


```{r}
final_tree <- finalize_model(tree_spec_tune, select_best(tree_rs)) #finalize model

#show the predictions from this final model
final_tree_fit <- last_fit(final_tree, 
                           user~.,
                           tracks_split)

final_tree_fit$.predictions

tree_metrics <- collect_metrics(final_tree_fit)

```

## Bagged Tree

```{r bagg_model}

#specify model without tuning parameters (using defaults)
bag_spec <- bag_tree() |> 
  set_engine("rpart", times = 50) |> 
  set_mode("classification")

#with tuning = takes 5ever
bag_spec_tune <- bag_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = tune()
) |> 
  set_engine("rpart", times = 50) |> 
  set_mode("classification")

#create hyperparameter search grid
bagrid <- grid_regular(
  cost_complexity(),
  tree_depth(),
  min_n(),
  levels = 5
)

#wrap up into a workflow
wf_bag_tune <- workflow() |> 
  add_recipe(tracks_recipe) |> 
  add_model(bag_spec_tune)

```

```{r bag_tune}

# #trying to make this faster by using more cores than default:
# ncores <- detectCores() - 1 #use one less than the number of cores on my machine
# registerDoParallel(cores = ncores) #register this number of cores
# cl <- makeCluster(ncores, type = "FORK") #make cluster with fork
# registerDoParallel(cl)

#using taylor to knit instead so hopefully basic function will work:
doParallel::registerDoParallel()

#fit workflow to resamples
bag_rs <- tune_grid(
  bag_spec_tune,
  user ~.,
  resamples = cv_folds,
  grid = bagrid,
  metrics = metric_set(accuracy)
)
# stopCluster(cl) #manually stop clusters
# stopImplicitCluster() #running into problems - hopefully this stops it better??

```


```{r bag_final}

final_bag <- finalize_model(bag_spec_tune, select_best(bag_rs)) #finalize model

#show the predictions from this final model
final_bag_fit <- last_fit(final_bag, 
                           user~.,
                           tracks_split)

final_bag_fit$.predictions #look at some predictions if you want if you care

bag_metrics <- collect_metrics(final_bag_fit)


```


## Random Forest

```{r rf_model}
#specify model

rand_spec_tune <- rand_forest(
  mtry = tune(), #using sqrt(p) = 4 for mtry
  trees = 1000, #using default number
  min_n = tune() 
) |> 
  set_engine("ranger") |> 
  set_mode("classification")

# put into a workflow
wf_rand_tune <- workflow() |> 
  add_recipe(tracks_recipe) |> 
  add_model(rand_spec_tune)

#train hyperparameters

#set up paralelle again
doParallel::registerDoParallel()
set.seed(321)
rand_tune_res <- tune_grid(
  wf_rand_tune,
  resamples = cv_folds,
  grid = 20 #testing 20 points to tune
)

#select the best parameters
rand_tune_best <- rand_tune_res |> 
  select_best("accuracy")


```


```{r rand_final}

final_rand <- finalize_model(rand_spec_tune, select_best(rand_tune_res)) #finalize model

#show the predictions from this final model
final_rand_fit <- last_fit(final_rand, 
                           user~.,
                           tracks_split)

final_rand_fit$.predictions #look at some predictions if you want if you care

rand_metrics <- collect_metrics(final_rand_fit)


```

# Comparing the models

Compare the performance of the four final models you have created.  

Use appropriate performance evaluation metric(s) for this classification task. A table would be a good way to display your comparison.  Use at least one visualization illustrating your model results
```{r model_metrics}

#compare metrics
knn_metrics

tree_metrics

bag_metrics

rand_metrics

#create table comparing accuracy metrics
accuracy_tibble <- tibble(
  model = c("knn", "decision_tree", "bagged_tree", "random_forest"),
  accuracy = c(round(knn_metrics[1, 3], 3), round(tree_metrics[1, 3], 3), round(bag_metrics[1, 3], 3), round(rand_metrics[1, 3], 3)),
  area_under_curve = c(round(knn_metrics[2, 3], 3), round(tree_metrics[2, 3], 3), round(bag_metrics[2, 3], 3), round(rand_metrics[2, 3], 3))
)

gt(accuracy_tibble)

```

```{r}
#visualize accuracy
ggplot(data = accuracy_tibble,
       aes(x = model,
           y = accuracy,
           fill = model)) +
  geom_col() +
  labs(
    title = "comparing the accuracy of four machine learning models"
  )


```

The greatest accuracy and the greatest area under the curve came from the random forest model. It's not surprising that this preformed much better than a single decision tree because a forest has lower bias than a single tree. The Bagging preformed similarly, and probably could have been tuned to preform even better than it did but the computation time trade off wasn't worth it. I'm a little surprised that the KNN model preformed worst, even worse than the single decision tree. I think this is because in the case of which songs belong to which user, the similarity of songs may not be the best predictor - especially becuase Jillian and I both seem to have similar music preferences in terms of the features used in this model. 



