---
title: "Lab 6"
author: "Erica Bishop"
date: "2023-03-01"
output: pdf_document
---

## Case Study Eel Species Distribution Modeling

This week's lab follows a modeling project described by Elith et al. (2008) (Supplementary Reading)

```{r libraries}
library(tidyverse)
library(tidymodels)
library(janitor)
library(tictoc)
library(xgboost)

```

```{r chunk_setup}
# set default chunk options
knitr::opts_chunk$set(fig.width = 4, fig.height = 3, 
                      echo = TRUE, message = FALSE, 
                      warning = FALSE, tidy.opts = list(width.cutoff = 60), 
                      tidy = TRUE)

```


## Data

Grab the model training data set from the class Git:

data/eel.model.data.csv

```{r}

eels <- read_csv(file = "https://raw.githubusercontent.com/MaRo406/eds-232-machine-learning/main/data/eel.model.data.csv") |> 
  clean_names()

```

### Split and Resample

Split the joined data from above into a training and test set, stratified by outcome score. Use 10-fold CV to resample the training set, stratified by Angaus

```{r}
set.seed(654) #set seed

#split data
eels_split <- initial_split(data = eels, #using default split
                            strata = angaus) #specifying outcome stratification
eels_train <- training(eels_split)
eels_test <- testing(eels_split)

#resample training data with 10-fold
eels_cv <- eels_train |> 
  vfold_cv(strata = angaus) #strata by outcome variable


```

### Preprocess

Create a recipe to prepare your data for the XGBoost model.  We are interested in predicting the binary outcome variable Angaus which indicates presence or absence of the eel species Anguilla australis

```{r}
eels_rec <- recipe(angaus ~ ., #using all predictors
                   data = eels_train) |> 
  step_dummy(method, one_hot = TRUE) |> #encode character variable
  step_normalize(all_numeric(), -all_outcomes()) |> #normalize numeric vars
  prep()

```

## Tuning XGBoost

### Tune Learning Rate

Following the XGBoost tuning strategy outlined on Monday, first we conduct tuning on just the learn_rate parameter:

1.  Create a model specification using {xgboost} for the estimation

-   Only specify one parameter to tune()

```{r}

eels_xgb_spec1 <- boost_tree(learn_rate = tune(),
                             trees = 3000 #set ntrees, use defaulst for rest
                             ) |> 
  set_engine("xgboost") |> 
  set_mode("classification")

#create tuning grid

#search grid of hyperparameter possibilities
#grid_max_entropy gives evenly spaced attempts - can be more efficient
xg_grid <- expand.grid(#feedsequence)

#tune in parallel
doParallel::registerDoParallel()

#tune the learn rate parameter
xg_tune_grid <- tune_grid(
  eels_xgb_spec1, #model specification
  angaus ~., #formula
  resamples = eels_cv, #resampling
  grid = xg_grid, #use search grid
  metrics = metric_set(accuracy)
  )

```


2.  Set up a grid to tune your model by using a range of learning rate parameter values: expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))

-   Use appropriate metrics argument(s) - Computational efficiency becomes a factor as models get more complex and data get larger. Record the time it takes to run. Do this for each tuning phase you run.You could use {tictoc} or Sys.time().

3.  Show the performance of the best models and the estimates for the learning rate parameter values associated with each.

### Tune Tree Parameters

1.  Create a new specification where you set the learning rate (which you already optimized) and tune the tree parameters.

2.  Set up a tuning grid. This time use grid_max_entropy() to get a representative sampling of the parameter space

3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.

### Tune Stochastic Parameters

1.  Create a new specification where you set the learning rate and tree parameters (which you already optimized) and tune the stochastic parameters.

2.  Set up a tuning grid. Use grid_max_entropy() again.

3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.

## Finalize workflow and make final prediction

1.  Assemble your final workflow will all of your optimized parameters and do a final fit.

2. How well did your model perform? What types of errors did it make?

## Fit your model the evaluation data and compare performance

1.  Now fit your final model to the big dataset: data/eval.data.csv

2.  How does your model perform on this data?

3.  How do your results compare to those of Elith et al.?

-   Use {vip} to compare variable importance
-   What do your variable importance results tell you about the distribution of this eel species?