---
title: "Lab 3 Demo"
author: "Erica Bishop"
date: "2023-01-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rsample)
library(skimr)
library(glmnet)
```

## Data Wrangling and Exploration
```{r data}
#load and inspect the data
ames <- AmesHousing::make_ames()

```

##Train a model
```{r intial_split}
# Stratified sampling with the rsample package
set.seed(123) #set a seed for reproducibility
split <- initial_split(data = ames,
                       prop = .70, #using 70% for training because for lab we'll be using smaller dataset (to leave more for test)
                       strata = "Sale_Price")  #stratify data so that each split has a representative distribution of specified variable
#strata are normally set based on outcome variable

split #object of class function - not super useful so use to split the data

ames_train <- training(split)
ames_test  <- testing(split)

#look at data sets:

head(ames_train)
head(ames_test)

#get some more info about our data (with skimr)
skim(ames_train)

```

```{r model_data}
#Create training feature matrices using model.matrix() (auto encoding of categorical variables)

X <- model.matrix(Sale_Price ~., ames_train)[,-1] #remove first column because its just an intercept

dim(X)

# transform y with log() transformation
#becuase sale price variable is highly grouped at lower end with fewer at the high end, this helps make regression easier
Y <- log(ames_train$Sale_Price)

```

```{r glmnet}
#fit a ridge model, passing X,Y,alpha to glmnet()
#alpha value of 0 is a ridge model, 1 is a lasso, values between 0 and 1 are an elastic net
#glmnet = generalized linear model - net for lasso / ridge
ridge <- glmnet(x = X, 
                y = Y,
                alpha = 0)

#plot() the glmnet model object

plot(ridge, xvar = "lambda")

#this plot shows how coefficients go to zero as lambda increases
#upper axis is constant because in a ridge regression there is no feature selection (so we are not reducing the number of features)
#plot shows log of lambda automatically becuase it makes it easier to visualize
  
```

```{r}
# lambdas applied to penalty parameter.  Examine the first few
ridge$lambda |> 
  head()


# small lambda results in large coefficients
coef(ridge)[c("Latitude", "Overall_QualVery_Excellent"), 100] #100 index corresponds to highest value of lambda

# what about for small coefficients?
coef(ridge)[c("Latitude", "Overall_QualVery_Excellent"), 1] 
#super small parameter values!!

```
How much improvement to our loss function as lambda changes?

##Tuning
```{r cv.glmnet}
# Apply CV ridge regression to Ames data.  Same arguments as before to glmnet()
ridge <- cv.glmnet(
  x = X,
  y = Y,
  alpha = 0
)

# Apply CV lasso regression to Ames data
lasso <- cv.glmnet(
  x = X,
  y = Y,
  alpha = 1
)

# plot results
par(mfrow = c(1, 2))
plot(ridge, main = "Ridge penalty\n\n")
plot(lasso, main = "Lasso penalty\n\n")


#see that lasso penalty changes lambda (selects features)
#first dotted line is minimum lamda value to minimize mean square error
#second dotted line is one standard error away from minimum
# want the model with the fewwest features that still within one standard error - "most parsimonious model"

```

10-fold CV MSE for a ridge and lasso model. What's the "rule of 1 standard deviation"?

In both models we see a slight improvement in the MSE as our penalty log(Î») gets larger, suggesting that a regular OLS model likely overfits the training data. But as we constrain it further (i.e., continue to increase the penalty), our MSE starts to increase. 

Let's examine the important parameter values apparent in the plots.
```{r}
# Ridge model
summary(ridge)

# minimum MSE



# lambda for this min MSE

# lambda for this MSE


# Lasso model
min(lasso$cvm)       # minimum MSE

lasso$lambda.min     # lambda for this min MSE


# 1-SE rule
lasso$lambda.1se  # lambda for this MSE

# No. of coef | 1-SE MSE

```

```{r}
# Ridge model
ridge_min <- glmnet(
  x = X,
  y = Y,
  alpha = 0
)

# Lasso model
lasso_min <- glmnet(
  x = X,
  y = Y,
  alpha = 1
)

par(mfrow = c(1, 2))
# plot ridge model
plot(ridge_min, xvar = "lambda", main = "Ridge penalty\n\n")
abline(v = log(ridge$lambda.min), col = "red", lty = "dashed")
abline(v = log(ridge$lambda.1se), col = "blue", lty = "dashed")

# plot lasso model
plot(lasso_min, xvar = "lambda", main = "Lasso penalty\n\n")
abline(v = log(lasso$lambda.min), col = "red", lty = "dashed")
abline(v = log(lasso$lambda.1se), col = "blue", lty = "dashed")
```


```{r }
skim(ames_train)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.