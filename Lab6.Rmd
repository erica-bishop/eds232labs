---
title: "Lab 6"
author: "Erica Bishop"
date: "2023-03-01"
output: html_document
---

## Case Study Eel Species Distribution Modeling

This week's lab follows a modeling project described by Elith et al. (2008) (Supplementary Reading)

```{r libraries}
library(tidyverse)
library(tidymodels)
library(janitor)
library(tictoc)
library(xgboost)
library(vip)

```
## Data

Grab the model training data set from the class Git:

data/eel.model.data.csv

```{r}

eels <- read_csv(file = "https://raw.githubusercontent.com/MaRo406/eds-232-machine-learning/main/data/eel.model.data.csv") |> 
  clean_names() |> 
  mutate(angaus = as.factor(angaus)) #make outcome variable a factor
  #theres probably a way to do this in the recipe but kept getting errors so its happening here

```

### Split and Resample

Split the joined data from above into a training and test set, stratified by outcome score. Use 10-fold CV to resample the training set, stratified by Angaus

```{r}
set.seed(654) #set seed

#split data
eels_split <- initial_split(data = eels, #using default 70/30 split
                            strata = angaus) #stratify bt outcome variable
eels_train <- training(eels_split)
eels_test <- testing(eels_split)

#resample training data with 10-fold
eels_cv <- eels_train |> 
  vfold_cv(strata = angaus) #strata by outcome variable


```

### Preprocess

Create a recipe to prepare your data for the XGBoost model.  We are interested in predicting the binary outcome variable Angaus which indicates presence or absence of the eel species Anguilla australis

```{r}
eels_rec <- recipe(angaus ~ ., #using all predictors
                   data = eels_train) |> 
  step_dummy(method, one_hot = TRUE) |> #encode character variable
  step_normalize(all_numeric(), -all_outcomes()) |> #normalize numeric vars
  prep()

#bake it (for the sake of tuning mtry later)
baked_eels <-
  eels_rec |> 
  bake(eels_train)

```

## Tuning XGBoost

### Tune Learning Rate

Following the XGBoost tuning strategy outlined on Monday, first we conduct tuning on just the learn_rate parameter:

1.  Create a model specification using {xgboost} for the estimation

-   Only specify one parameter to tune()

```{r}
#specify model parameters (just learn rate to start)
eels_xgb_spec1 <- boost_tree(learn_rate = tune()
                             ) |> 
  set_engine("xgboost") |> 
  set_mode("classification")

#put into workflow
eels_wf1 <- workflow() |> 
  add_model(eels_xgb_spec1) |> 
  add_recipe(eels_rec)


```


2.  Set up a grid to tune your model by using a range of learning rate parameter values: expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))

-   Use appropriate metrics argument(s) - Computational efficiency becomes a factor as models get more complex and data get larger. Record the time it takes to run. Do this for each tuning phase you run.You could use {tictoc} or Sys.time().

```{r}
#create grid
tune_grid <- expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))

tic() #start timer
#tune in parallel
doParallel::registerDoParallel()
# Fit the workflow on our predefined folds and hyperparameters
fit_eels1 <- eels_wf1 |> 
  tune_grid( 
    eels_cv,
    grid = tune_grid,
    metrics = metric_set(accuracy))
toc() #end timer


```

3.  Show the performance of the best models and the estimates for the learning rate parameter values associated with each.

```{r}
#collect metrics
eels1_metrics <- fit_eels1 |> 
  collect_metrics()

#visually check out the accuracy
autoplot(fit_eels1) 
#looks like as learning rate increases the accuracy increases

```
```{r}

show_best(fit_eels1) #show me the best tree! 
select_best(fit_eels1) #pick the best tree

best_learn_rate <- as.numeric(select_best(fit_eels1)[1,1]) #store optimal value as number

```

### Tune Tree Parameters

1.  Create a new specification where you set the learning rate (which you already optimized) and tune the tree parameters.

```{r}

#specify model new parameters with optimal learn rate
eels_xgb_spec2 <- boost_tree(learn_rate = best_learn_rate,
                             trees = 3000,
                             tree_depth = tune(),
                             min_n = tune(),
                             loss_reduction = tune()
                             ) |> 
  set_engine("xgboost") |> 
  set_mode("classification")

#put into workflow
eels_wf2 <- workflow() |> 
  add_model(eels_xgb_spec2) |> 
  add_recipe(eels_rec)


```


2.  Set up a tuning grid. This time use grid_max_entropy() to get a representative sampling of the parameter space

```{r}

#set up parameters to feed to the grid
xgb_params <- parameters( #list same parameters with tune() in spec
  tree_depth(),
  min_n(),
  loss_reduction()
)

#create grid
grid_2 <- grid_max_entropy(
  xgb_params,
  size = 60 #total number of parameter value combos
)

tic() #start timer
#tune in parallel
doParallel::registerDoParallel()
# Fit the workflow on our predefined folds and hyperparameters
fit_eels2 <- eels_wf2 |> 
  tune_grid( 
    eels_cv,
    grid = grid_2,
    metrics = metric_set(accuracy))
toc() #end timer


```

3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.

```{r}

#collect metrics
eels2_metrics <- fit_eels2 |> 
  collect_metrics()

#visually check out the accuracy
autoplot(fit_eels2) 
#YIKES - These are all over the place 
#is it becuase of the way the three are combined? Why is there no trend?

```
```{r}
show_best(fit_eels2) #show me the best tree

#store best tuning parameters
best_min_n <- as.numeric(select_best(fit_eels2)[1,1])
  
best_tree_depth <- as.numeric(select_best(fit_eels2)[1,2])
  
best_loss <- as.numeric(select_best(fit_eels2)[1,3])

```


### Tune Stochastic Parameters

1.  Create a new specification where you set the learning rate and tree parameters (which you already optimized) and tune the stochastic parameters.

```{r}

#specify model new optimized parameters
eels_xgb_spec3 <- boost_tree(learn_rate = best_learn_rate,
                             trees = 3000,
                             tree_depth = best_tree_depth,
                             min_n = best_min_n,
                             loss_reduction = best_loss,
                             mtry = tune(),
                             sample_size = tune()
                             ) |> 
  set_engine("xgboost") |> 
  set_mode("classification")

#put into workflow
eels_wf3 <- workflow() |> 
  add_model(eels_xgb_spec3) |> 
  add_recipe(eels_rec)

```


2.  Set up a tuning grid. Use grid_max_entropy() again.

```{r}

#set up parameters to feed to the grid
xgb_params2 <- parameters( #list same parameters with tune() in spec
  finalize(mtry(), select(baked_eels, -angaus)), #mtry needs to be determined based on data (p)
  sample_size = sample_prop(
    c(0.4, 0.9) #requires proportion rather than integer
  )
)

#create grid
grid_3 <- grid_max_entropy(
  xgb_params2,
 size =  60 #trying larger size to see if this improves 
 #larger size improves performance but slower computation
)

tic() #start timer
#tune in parallel
doParallel::registerDoParallel()
# Fit the workflow on our predefined folds and hyperparameters
fit_eels3 <- eels_wf3 |> 
  tune_grid( 
    eels_cv,
    grid = grid_3,
    metrics = metric_set(accuracy))
toc() #end timer


```


3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.

```{r}
#visually check out the accuracy
autoplot(fit_eels3) 

```

```{r}


show_best(fit_eels3) #show me the best tree

#store best tuning parameters
best_mtry <- as.numeric(select_best(fit_eels3)[1,1])
  
best_samplesize <- as.numeric(select_best(fit_eels3)[1,2])
  
```


## Finalize workflow and make final prediction

1.  Assemble your final workflow will all of your optimized parameters and do a final fit.

```{r}
#create final model specification with tuned parameters
eels_final_spec <- boost_tree(
    learn_rate = best_learn_rate,
    trees = 3000,
    tree_depth = best_tree_depth,
    min_n = best_min_n,
    loss_reduction = best_loss,
    mtry = best_mtry,
    sample_size = best_samplesize
) |> 
  set_engine("xgboost") |> 
  set_mode("classification")

#Final workflow
final_wf <- workflow() |> 
  add_model(eels_final_spec) |> 
  add_recipe(eels_rec)

#maybe this will work better:
final_model <- finalize_model(eels_final_spec, select_best(fit_eels3))

#Do a final fit and show predictions
final_eels_fit <- last_fit(
  final_model,
  angaus ~.,
  split = eels_split
)

final_eels_fit$.predictions #look at some predictions on the test data

eel_metrics <- collect_metrics(final_eels_fit)

gt::gt(eel_metrics) #print out table of metrics

```

2. How well did your model perform? What types of errors did it make?

The overall accuracy of my model was 84.46% accurate predictions for the presence of angaus eel, and the area under the curve os 0.867, which is a pretty good performance! (The closer to 1, the better). As you can see in the heatmap below, the most common type of error was false negative (predicting a value of 0 for no eels, when there were in fact eels present). The false negative rate was higher than the false positive rate for this model, I think because the majority class was the negative class (0, or an absence of eels). 

```{r}
#put predictions into dataframe
eel_preds <- as.data.frame(final_eels_fit$.predictions) |> 
  select(c(.pred_class, angaus))

#show the confustion matrix
conf_mat(eel_preds,
         truth = angaus,
         estimate = .pred_class) |> 
  autoplot(type = "heatmap")

```


## Fit your model the evaluation data and compare performance

1.  Now fit your final model to the big dataset: data/eval.data.csv

```{r}

#read in eval data
eels_eval_data <- read_csv(file = "https://raw.githubusercontent.com/MaRo406/eds-232-machine-learning/main/data/eel.eval.data.csv") |> 
  clean_names() |>  
  rename(angaus = angaus_obs) #rename outcome to match

#split this data
eels_eval_split <- initial_split(eels_eval_data)
eels_eval_train <- training(eels_eval_split)
eels_eval_test <- testing(eels_eval_split)

#fit model to the new data!
eval_eels_fit <- last_fit(
  final_wf,
  angaus ~.,
  split = eels_split
)


```


2.  How does your model perform on this data?

```{r}

eval_eels_fit$.predictions #look at some predictions on the test data

eval_metrics <- collect_metrics(eval_eels_fit)

gt::gt(eval_metrics) #print out table of metrics

```

```{r}
#put predictions into dataframe
eval_preds <- as.data.frame(eval_eels_fit$.predictions) |> 
  select(c(.pred_class, angaus))

#show the confustion matrix
conf_mat(eval_preds,
         truth = angaus,
         estimate = .pred_class) |> 
  autoplot(type = "heatmap")


```
The model performed similarly on the evaluation data, with 82.9% accuracy and an ROC value of 0.87. As you can see in the two confustion matrices, the models made very similar types of errors with the false negative rate being higher than the false positive rate. 

3.  How do your results compare to those of Elith et al.?

-   Use {vip} to compare variable importance
-   What do your variable importance results tell you about the distribution of this eel species?

```{r}

final_eels_fit |> 
  extract_fit_parsnip() |> 
  vip(geom = "point",
      num_features = 13) +
  labs(title = "Variable Importance in XGBoost Model") +
  theme_minimal()


```

`seg_sum_t`, which is the summer air temperature, had a much higher importance score than the other variables, including the winter air temperature (`seg_t_seas`). This indicates to me that the summer air temperature has a high influence on the success of the eels lifecycle and breeding season - moreso than the winter air temperatures. This was also the most important predictor identified in  Elith et al. 

The next most important predictors were areas with higher indigenous forest proportions (`us_native`), the site, and the distance to the coast, `ds_dist`. It's interesting that the presence of indigenous forests is so important for eel populations - this is probably because there are more complex ecosystem relationships between the properties of indigenous forests (biodiversity, lack of invasive species, etc.). 

Elith et al. also identified the proportion of native forests and the distance to coast as key predictors, along with bed sediment and maximum slope (`loc_sed` and `us_slope`). However, the paper also included fishing method as an important predictor, which was one of the least important predictors in my model. I think this is likely because when I encoded the fishing method, it split the predictor into multiple variables for the types of fishing methods, which perhaps diluted the predictive power of the variable. 

